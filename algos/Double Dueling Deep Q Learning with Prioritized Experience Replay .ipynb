{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import math\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")\n",
    "\n",
    "from lib import plotting\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.envs.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(frame_idx, rewards, losses):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"\n",
    "    Q-Value Estimator neural network.\n",
    "\n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sess ,state_dim, action_space, scope=\"estimator\", summaries_dir='./', deulling=False):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph.\n",
    "        \"\"\"\n",
    "        self.summary_writer = None\n",
    "        \n",
    "        with tf.variable_scope(scope):\n",
    "            self.scope = scope\n",
    "            # Placeholders for our input\n",
    "            self.X_pl = tf.placeholder(shape=(None, state_dim), dtype=tf.float32, name=\"X\")\n",
    "            # The TD target value\n",
    "            self.y_pl = tf.placeholder(shape=(None,), dtype=tf.float32, name=\"y\")\n",
    "            \n",
    "            self.weights = tf.placeholder(shape=(None,), dtype=tf.float32, name=\"weights\")\n",
    "            \n",
    "            self.actions_pl = tf.placeholder(shape=(None,), dtype=tf.int32, name=\"act\")\n",
    "            \n",
    "            #batch_size = tf.shape(self.X_pl)[0]\n",
    "            \n",
    "            with tf.name_scope('fcl1'):\n",
    "                self.fcl1 = tf.layers.dense( self.X_pl, 128, activation=tf.nn.relu, name='fcl1')\n",
    "\n",
    "            # common layer between advantage and value networks\n",
    "            if deulling is True:\n",
    "                # value prediction layers\n",
    "                with tf.name_scope('fcl2_value'):\n",
    "                    self.fcl2_value = tf.layers.dense( self.fcl1, 128, activation=tf.nn.relu, name='fcl2_value' )\n",
    "\n",
    "                with tf.name_scope('fcl3_value'):\n",
    "                    self.value_prediction = tf.layers.dense( self.fcl2_value, 1, name='fcl3_value')\n",
    "\n",
    "                # advantage prediction layers   \n",
    "                with tf.name_scope('fcl2_advantage'):\n",
    "                    self.fcl2_advantage = tf.layers.dense( self.fcl1, 128, activation=tf.nn.relu, name='fcl2_advantage')\n",
    "\n",
    "                with tf.name_scope('fcl3_advantage'):\n",
    "                    self.advantage_prediction = tf.layers.dense( self.fcl2_advantage, action_space, name='fcl3_advantage')\n",
    "\n",
    "                self.mean_advantage = tf.reduce_mean(tf.reduce_mean(self.advantage_prediction))\n",
    "                self.predictions = self.value_prediction + (self.advantage_prediction - self.mean_advantage)\n",
    "            \n",
    "            else:                \n",
    "                with tf.name_scope('fcl2'):\n",
    "                    self.fcl2 = tf.layers.dense( self.fcl1, 128, activation=tf.nn.relu )\n",
    "                \n",
    "                with tf.name_scope('fcl3'):\n",
    "                    self.predictions = tf.layers.dense( self.fcl2, action_space )\n",
    "\n",
    "            self.indices = tf.concat((tf.expand_dims(tf.range(32), 1), tf.expand_dims(self.actions_pl, 1)), axis=1)\n",
    "            self.state_action_predictions = tf.gather_nd(self.predictions, self.indices)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            with tf.name_scope('loss'):\n",
    "                self.losses = tf.squared_difference(self.y_pl, self.state_action_predictions)\n",
    "                self.weighted_losses = self.losses * self.weights\n",
    "                self.prios = self.weighted_losses + tf.constant(1e-5)\n",
    "                self.loss = tf.reduce_mean(self.weighted_losses)\n",
    "\n",
    "            # Optimizer and training\n",
    "            with tf.name_scope('train'):\n",
    "                self.optimizer = tf.train.AdamOptimizer()\n",
    "                self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "            tf.summary.scalar('loss', self.loss)\n",
    "            \n",
    "            # Summaries for Tensorboard\n",
    "            self.summaries = tf.summary.merge([\n",
    "                tf.summary.scalar(\"loss\", self.loss),\n",
    "                tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "                tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "                tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "            ])\n",
    "            \n",
    "            summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "            if not os.path.exists(summary_dir):\n",
    "                os.makedirs(summary_dir)\n",
    "            self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "            self.summary_writer.add_graph(sess.graph)\n",
    "\n",
    "    \n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, { self.X_pl: s })\n",
    "\n",
    "    def update(self, sess, s, a, y, w, replay, buf_ptrs):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        feed_dict = { self.X_pl: s, self.y_pl: y , self.actions_pl: a, self.weights: w }\n",
    "        summaries, global_step, loss, prios,values, mean,  _ = sess.run(\n",
    "            [self.summaries, tf.contrib.framework.get_global_step(), self.loss, self.prios, self.value_prediction,self.mean_advantage,  self.train_op],\n",
    "            feed_dict)\n",
    "        replay.update(buf_ptrs, prios)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriorityBuffer():\n",
    "    \"\"\"\n",
    "    A quick and dirty Priority Buffer implementation using numpy ()\n",
    "\n",
    "    Args:\n",
    "      capacity: capacity of priority buffer \n",
    "      action_dim: dimension of actions (usually just 1 for openai envs)\n",
    "      state_dim: dimension of states\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity, action_dim, state_dim):\n",
    "        self.capacity = capacity\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = 1\n",
    "        self.reward_dim = 1\n",
    "        self.priority_dim = 1\n",
    "        self.done_dim = 1\n",
    "        self.buffer = np.zeros(( capacity, self.priority_dim + self.state_dim + self.action_dim + self.reward_dim + self.state_dim + self.done_dim ), dtype=np.float64)\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Adds a state, action, reward, next_state tuple into buffer\n",
    "        Args: \n",
    "        state: current state \n",
    "        action: action taken in current state\n",
    "        reward: immediate reward recieved from current action \n",
    "        next_state: next state the agent transitions into after taking action \n",
    "        done: indicates whether the current episode is done\n",
    "        \"\"\"\n",
    "        # assign highest priority to any new transitions\n",
    "        priority = np.max(self.buffer[:, 0]) if self.size > 0 else 1.0\n",
    "        self.buffer[self.ptr, :] = np.concatenate((np.array([priority]), state, np.array([action, reward]), next_state, np.array([done])), axis=0)\n",
    "        \n",
    "        self.ptr = (self.ptr + 1) % self.capacity\n",
    "        if self.size < self.capacity: \n",
    "            self.size += 1\n",
    "    \n",
    "    \n",
    "    def update(self, buffer_ptrs, priorities):\n",
    "        \"\"\"\n",
    "        Utility function for updating all the priorities of the priority buffer (called after new TD errors )\n",
    "        \"\"\"\n",
    "        self.buffer[buffer_ptrs, 0] = priorities\n",
    "    \n",
    "    # return associated buffer pointers for easy update op\n",
    "    def sample(self, batch_size, alpha , beta):\n",
    "        \"\"\"\n",
    "        Used for sampling the priority buffer via prioritized sampling\n",
    "        \n",
    "        Args: \n",
    "        batch_size: the size of the training batch\n",
    "        beta: a weight used to adjust the importance sampling weights\n",
    "        \n",
    "        \"\"\"\n",
    "        # secondary values used for computation\n",
    "        priority_sum = np.sum(self.buffer[:self.size, 0])\n",
    "        max_priority = np.max(self.buffer[:self.size, 0])\n",
    "        \n",
    "        # prioritized sampling\n",
    "        probs = ( self.buffer[:self.size, 0] ) / priority_sum\n",
    "        sample_indices = np.random.choice(self.size, batch_size, p=probs )\n",
    "        states = self.buffer[sample_indices, 1:1 + self.state_dim]\n",
    "        actions = self.buffer[sample_indices, 1 + self.state_dim:1 + self.state_dim + self.action_dim]\n",
    "        rewards = self.buffer[sample_indices, 1 + self.state_dim + self.action_dim:1 + self.state_dim + self.action_dim + 1]\n",
    "        next_states = self.buffer[sample_indices, self.state_dim+1+self.action_dim+1: self.state_dim * 2 + 1 + self.action_dim + 1]\n",
    "        dones = self.buffer[sample_indices, self.state_dim * 2 + 1 + self.action_dim + 1 :]\n",
    "        # importance sampling weights\n",
    "        weights  = ( ( self.size * probs[sample_indices] ) ** ( -beta ) ) \n",
    "        weights /= np.max(weights)\n",
    "        return sample_indices, states, actions.squeeze(), rewards.squeeze(), next_states, dones.squeeze(), weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_model_parameters(sess, estimator1, estimator2):\n",
    "    \"\"\"\n",
    "    Copies the model parameters of one estimator to another.\n",
    "\n",
    "    Args:\n",
    "      sess: Tensorflow session instance\n",
    "      estimator1: Estimator to copy the paramters from\n",
    "      estimator2: Estimator to copy the parameters to\n",
    "    \"\"\"\n",
    "    e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "    e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "    e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "    e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "    update_ops = []\n",
    "    for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "        op = e2_v.assign(e1_v)\n",
    "        update_ops.append(op)\n",
    "    sess.run(update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    \n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_q_learning(sess,\n",
    "                    env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    n_steps,\n",
    "                    replay_memory_init_size=64,\n",
    "                    replay_memory_size=10000,\n",
    "                    update_target_estimator_every=100,\n",
    "                    discount_factor=0.99,\n",
    "                    batch_size=32):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for off-policy TD control using Function Approximation and Priorized Importance sampling.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        sess: Tensorflow Session object\n",
    "        env: OpenAI environment\n",
    "        q_estimator: Estimator object used for the q values\n",
    "        target_estimator: Estimator object used for the targets\n",
    "        n_steps: number of steps to run for\n",
    "        replay_memory_size: Size of the replay memory\n",
    "        replay_memory_init_size: Number of random experiences to sampel when initializing \n",
    "          the reply memory.\n",
    "        update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "          target estimator every N steps\n",
    "        discount_factor: Gamma discount factor\n",
    "        epsilon_start: Chance to sample a random action when taking an action.\n",
    "          Epsilon is decayed over time and this is the start value\n",
    "        batch_size: Size of batches to sample from the replay memory\n",
    "\n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    # The replay memory\n",
    "    replay_memory = PriorityBuffer(replay_memory_size, 1, env.observation_space.shape[0])\n",
    "    \n",
    "    writer = tf.summary.FileWriter(\"/tmp/rl_research/l\")\n",
    "    writer.add_graph(sess.graph)\n",
    "    \n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(10000),\n",
    "        episode_rewards=np.zeros(10000))\n",
    "    \n",
    "    # epsilon decay tracking\n",
    "    epsilon_start = 1.0\n",
    "    epsilon_final = 0.01\n",
    "    epsilon_decay = 1000\n",
    "\n",
    "    epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)\n",
    "\n",
    "    \n",
    "    # The epsilon decay schedule\n",
    "    epsilons = [epsilon_by_frame(i) for i in range(10000)]\n",
    "    betas = np.linspace(0.4, 1.0, 1000)\n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(q_estimator, env.action_space.n)\n",
    "\n",
    "    # Populate the replay memory with initial experience\n",
    "    print(\"Populating replay memory...\")\n",
    "    \n",
    "    done = True\n",
    "    # Get the current time step\n",
    "    total_t = sess.run(tf.contrib.framework.get_global_step())\n",
    "    \n",
    "    losses = []\n",
    "    for i in range(replay_memory_init_size):\n",
    "        epsilon = epsilons[min(total_t, len(epsilons)-1)]\n",
    "        if done: \n",
    "            state = env.reset()\n",
    "            action = np.random.choice(range(env.action_space.n), p=policy(sess, state, epsilon))\n",
    "        else:\n",
    "            action = np.random.choice(range(env.action_space.n), p=policy(sess, state, epsilon))\n",
    "            state = next_state\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        replay_memory.add(state, action, reward, next_state, done)\n",
    "        total_t += 1\n",
    "        \n",
    "    loss = None\n",
    "    print ('Finished Init Repo Memory')\n",
    "    i_episode = 0\n",
    "    for step in range(n_steps):\n",
    "\n",
    "        # Reset the environment\n",
    "        if done: \n",
    "            state = env.reset()\n",
    "            # Add summaries to tensorboard\n",
    "            episode_summary = tf.Summary()\n",
    "            episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], node_name=\"episode_reward\", tag=\"episode_reward\")\n",
    "            episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], node_name=\"episode_length\", tag=\"episode_length\")\n",
    "            q_estimator.summary_writer.add_summary(episode_summary, total_t)\n",
    "            q_estimator.summary_writer.flush()\n",
    "            yield total_t, plotting.EpisodeStats(\n",
    "                episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "                episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "            i_episode += 1\n",
    "        \n",
    "\n",
    "        epsilon = epsilons[min(total_t, len(epsilons)-1)]\n",
    "\n",
    "\n",
    "        if total_t % update_target_estimator_every == 0:\n",
    "            copy_model_parameters (sess ,q_estimator ,target_estimator)\n",
    "\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "        action = np.random.choice(range(env.action_space.n), p=policy(sess, state, epsilon))\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        replay_memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        buffer_ptrs, minibatch_states, minibatch_actions, minibatch_rewards, minibatch_next_states, minibatch_dones, minibatch_weights = replay_memory.sample(batch_size, 0.6, betas[min(len(betas) - 1,i_episode)])\n",
    "        \n",
    "        # ****DOUBLE DQN COMES HERE****\n",
    "        # decoupling action selection (using q_estimator) from action evaluation (using target_estimator)\n",
    "        max_actions = np.argmax(q_estimator.predict(sess, minibatch_next_states), axis=1)\n",
    "        targets = minibatch_rewards + (1 - minibatch_dones) * discount_factor * target_estimator.predict(sess, minibatch_next_states)[np.arange(len(minibatch_actions)), max_actions]\n",
    "        #print (1 - minibatch_dones)\n",
    "        # use loss and do backprop update on neural network estimator\n",
    "        loss = q_estimator.update(sess, minibatch_states, minibatch_actions, targets, minibatch_weights, replay_memory, buffer_ptrs)\n",
    "        losses.append(loss)\n",
    "\n",
    "        stats.episode_rewards[i_episode] += reward\n",
    "        stats.episode_lengths[i_episode] = step\n",
    "\n",
    "        state = next_state\n",
    "        total_t += 1\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating replay memory...\n",
      "Finished Init Repo Memory\n",
      "\n",
      "Episode Reward: 11.0\n",
      "\n",
      "Episode Reward: 15.0\n",
      "\n",
      "Episode Reward: 23.0\n",
      "\n",
      "Episode Reward: 15.0\n",
      "\n",
      "Episode Reward: 13.0\n",
      "\n",
      "Episode Reward: 19.0\n",
      "\n",
      "Episode Reward: 19.0\n",
      "\n",
      "Episode Reward: 17.0\n",
      "\n",
      "Episode Reward: 14.0\n",
      "\n",
      "Episode Reward: 21.0\n",
      "\n",
      "Episode Reward: 45.0\n",
      "\n",
      "Episode Reward: 17.0\n",
      "\n",
      "Episode Reward: 11.0\n",
      "\n",
      "Episode Reward: 11.0\n",
      "\n",
      "Episode Reward: 9.0\n",
      "\n",
      "Episode Reward: 23.0\n",
      "\n",
      "Episode Reward: 22.0\n",
      "\n",
      "Episode Reward: 11.0\n",
      "\n",
      "Episode Reward: 17.0\n",
      "\n",
      "Episode Reward: 17.0\n",
      "\n",
      "Episode Reward: 23.0\n",
      "\n",
      "Episode Reward: 35.0\n",
      "\n",
      "Episode Reward: 18.0\n",
      "\n",
      "Episode Reward: 12.0\n",
      "\n",
      "Episode Reward: 19.0\n",
      "\n",
      "Episode Reward: 28.0\n",
      "\n",
      "Episode Reward: 40.0\n",
      "\n",
      "Episode Reward: 46.0\n",
      "\n",
      "Episode Reward: 44.0\n",
      "\n",
      "Episode Reward: 14.0\n",
      "\n",
      "Episode Reward: 80.0\n",
      "\n",
      "Episode Reward: 10.0\n",
      "\n",
      "Episode Reward: 10.0\n",
      "\n",
      "Episode Reward: 40.0\n",
      "\n",
      "Episode Reward: 13.0\n",
      "\n",
      "Episode Reward: 12.0\n",
      "\n",
      "Episode Reward: 12.0\n",
      "\n",
      "Episode Reward: 103.0\n",
      "\n",
      "Episode Reward: 35.0\n",
      "\n",
      "Episode Reward: 42.0\n",
      "\n",
      "Episode Reward: 200.0\n",
      "\n",
      "Episode Reward: 88.0\n",
      "\n",
      "Episode Reward: 127.0\n",
      "\n",
      "Episode Reward: 74.0\n",
      "\n",
      "Episode Reward: 67.0\n",
      "\n",
      "Episode Reward: 200.0\n",
      "\n",
      "Episode Reward: 133.0\n",
      "\n",
      "Episode Reward: 168.0\n",
      "\n",
      "Episode Reward: 153.0\n",
      "\n",
      "Episode Reward: 116.0\n",
      "\n",
      "Episode Reward: 112.0\n",
      "\n",
      "Episode Reward: 151.0\n",
      "\n",
      "Episode Reward: 200.0\n",
      "\n",
      "Episode Reward: 200.0\n",
      "\n",
      "Episode Reward: 200.0\n",
      "\n",
      "Episode Reward: 101.0\n",
      "\n",
      "Episode Reward: 113.0\n",
      "\n",
      "Episode Reward: 110.0\n",
      "\n",
      "Episode Reward: 107.0\n",
      "\n",
      "Episode Reward: 94.0\n",
      "\n",
      "Episode Reward: 129.0\n",
      "\n",
      "Episode Reward: 135.0\n",
      "\n",
      "Episode Reward: 200.0\n",
      "\n",
      "Episode Reward: 200.0\n",
      "\n",
      "Episode Reward: 79.0\n",
      "\n",
      "Episode Reward: 95.0\n",
      "\n",
      "Episode Reward: 92.0\n",
      "\n",
      "Episode Reward: 126.0\n",
      "\n",
      "Episode Reward: 97.0\n",
      "\n",
      "Episode Reward: 87.0\n",
      "\n",
      "Episode Reward: 103.0\n",
      "\n",
      "Episode Reward: 93.0\n",
      "\n",
      "Episode Reward: 87.0\n",
      "\n",
      "Episode Reward: 82.0\n",
      "\n",
      "Episode Reward: 86.0\n",
      "\n",
      "Episode Reward: 84.0\n",
      "\n",
      "Episode Reward: 84.0\n",
      "\n",
      "Episode Reward: 77.0\n",
      "\n",
      "Episode Reward: 83.0\n",
      "\n",
      "Episode Reward: 101.0\n",
      "\n",
      "Episode Reward: 102.0\n",
      "\n",
      "Episode Reward: 90.0\n",
      "\n",
      "Episode Reward: 98.0\n",
      "\n",
      "Episode Reward: 101.0\n",
      "\n",
      "Episode Reward: 96.0\n",
      "\n",
      "Episode Reward: 91.0\n",
      "\n",
      "Episode Reward: 95.0\n",
      "\n",
      "Episode Reward: 119.0\n",
      "\n",
      "Episode Reward: 102.0\n",
      "\n",
      "Episode Reward: 115.0\n",
      "\n",
      "Episode Reward: 110.0\n",
      "\n",
      "Episode Reward: 122.0\n",
      "\n",
      "Episode Reward: 151.0\n",
      "\n",
      "Episode Reward: 123.0\n",
      "\n",
      "Episode Reward: 122.0\n",
      "\n",
      "Episode Reward: 152.0\n",
      "\n",
      "Episode Reward: 23.0\n",
      "\n",
      "Episode Reward: 153.0\n",
      "\n",
      "Episode Reward: 122.0\n",
      "\n",
      "Episode Reward: 125.0\n",
      "\n",
      "Episode Reward: 163.0\n",
      "\n",
      "Episode Reward: 177.0\n",
      "\n",
      "Episode Reward: 182.0\n",
      "\n",
      "Episode Reward: 192.0\n",
      "\n",
      "Episode Reward: 188.0\n",
      "\n",
      "Episode Reward: 200.0\n",
      "\n",
      "Episode Reward: 152.0\n",
      "\n",
      "Episode Reward: 12.0\n",
      "\n",
      "Episode Reward: 10.0\n",
      "\n",
      "Episode Reward: 9.0\n",
      "\n",
      "Episode Reward: 12.0\n",
      "\n",
      "Episode Reward: 11.0\n",
      "\n",
      "Episode Reward: 121.0\n",
      "\n",
      "Episode Reward: 20.0\n",
      "\n",
      "Episode Reward: 22.0\n",
      "\n",
      "Episode Reward: 22.0\n",
      "\n",
      "Episode Reward: 104.0\n",
      "\n",
      "Episode Reward: 154.0\n",
      "\n",
      "Episode Reward: 191.0\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "experiment_dir_q = os.path.abspath('./experiments')\n",
    "experiment_dir_target = os.path.abspath('./experiments_target')\n",
    "\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "\n",
    "# Run it!\n",
    "with tf.Session() as sess:\n",
    "    # Create estimators\n",
    "    q_estimator = Estimator(sess, env.observation_space.shape[0], env.action_space.n, scope='q_estimator', summaries_dir=experiment_dir_q, deulling=True)\n",
    "    target_estimator = Estimator(sess, env.observation_space.shape[0], env.action_space.n, scope='target_estimator', summaries_dir=experiment_dir_target, deulling=True)\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    for t, stats in deep_q_learning(sess,\n",
    "                        env,\n",
    "                        q_estimator=q_estimator,\n",
    "                        target_estimator=target_estimator,\n",
    "                        n_steps=10000,\n",
    "                        replay_memory_size=1000,\n",
    "                        replay_memory_init_size=64,\n",
    "                        update_target_estimator_every=100,\n",
    "                        discount_factor=0.99,\n",
    "                        batch_size=32):\n",
    "\n",
    "        print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))\n",
    "        #pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
